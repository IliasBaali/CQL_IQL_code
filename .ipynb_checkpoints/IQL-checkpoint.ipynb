{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements Implicit Q-Learning to solve Maze 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/ibaali3/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x155551008f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from src.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    eval_policy,\n",
    "    demo_policy,\n",
    "    plot_returns,\n",
    "    save_frames_as_gif,\n",
    "    return_range\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import einops\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from src.d4rl_dataset import D4RLSampler\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing D4RL, a library that contains training data obtained by running policy of different levels on a few environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/ibaali3/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "/home/hice1/ibaali3/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages/glfw/__init__.py:914: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Nov 28 2023 23:45:17\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-10 16:09:40.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils\u001b[0m:\u001b[36mget_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mUsing cuda device.\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:40.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils\u001b[0m:\u001b[36mset_seed\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mRandom seed set as 42.\u001b[0m\n",
      "/home/hice1/ibaali3/.conda/envs/cs8803drl_hw2/lib/python3.10/site-packages/gym/envs/registration.py:511: UserWarning: \u001b[33mWARN: Using the latest versioned environment `maze2d-umaze-v1` instead of the unversioned environment `maze2d-umaze`\u001b[0m\n",
      "  logger.warn(\n",
      "\u001b[32m2024-12-10 16:09:40.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mAction Dimension: 2\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:40.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mAction High: [1. 1.]\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:40.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mAction Low: [-1. -1.]\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:40.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mState Dimension: 4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "SEED: int = 42\n",
    "ENVIRONMENT_NAME: str='maze2d-umaze'\n",
    "\n",
    "# torch related defaults\n",
    "DEVICE = get_device()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Use random seeds for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "# instantiate the environment\n",
    "env = gym.make(ENVIRONMENT_NAME)\n",
    "\n",
    "# get the state and action dimensions\n",
    "action_dimension = env.action_space.shape[0]\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "\n",
    "logger.info(f'Action Dimension: {action_dimension}')\n",
    "logger.info(f'Action High: {env.action_space.high}')\n",
    "logger.info(f'Action Low: {env.action_space.low}')\n",
    "logger.info(f'State Dimension: {state_dimension}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to download the training data using D4RL library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:00<00:00,  8.19it/s]\n",
      "\u001b[32m2024-12-10 16:09:45.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mDataset type: <class 'dict'>\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:45.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mDataset keys: dict_keys(['observations', 'actions', 'next_observations', 'rewards', 'terminals'])\u001b[0m\n",
      "\u001b[32m2024-12-10 16:09:45.717\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m# Samples: 987540\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = d4rl.qlearning_dataset(env)\n",
    "\n",
    "logger.info(f'Dataset type: {type(dataset)}')\n",
    "logger.info(f'Dataset keys: {dataset.keys()}')\n",
    "logger.info(f'# Samples: {len(dataset[\"observations\"])}')\n",
    "sampler = D4RLSampler(dataset, 256, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the Q network that calculates the state-action values. Here, we use a double Q-network, as was highlighted in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dimension, action_dimension, hidden_dim, n_hidden):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.Q1 = nn.Sequential(\n",
    "            nn.Linear(state_dimension+action_dimension,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            *nn.ModuleList([nn.Linear(hidden_dim,hidden_dim),nn.ReLU()]*n_hidden),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "       \n",
    "        # Q2 architecture\n",
    "        self.Q2 = nn.Sequential(\n",
    "            nn.Linear(state_dimension+action_dimension,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            *nn.ModuleList([nn.Linear(hidden_dim,hidden_dim),nn.ReLU()]*n_hidden),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "        q1 = self.Q1(xu)\n",
    "        q2 = self.Q2(xu)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNetwork(nn.Module):\n",
    "    def __init__(self, state_dimension, hidden_dim, n_hidden):\n",
    "        super(VNetwork, self).__init__()\n",
    "\n",
    "        self.V = nn.Sequential(\n",
    "            nn.Linear(state_dimension,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            *nn.ModuleList([nn.Linear(hidden_dim,hidden_dim),nn.ReLU()]*n_hidden),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        v = self.V(state)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-10 16:09:46.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils\u001b[0m:\u001b[36mget_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mUsing cuda device.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "DEVICE = get_device()\n",
    "HIDDEN_DIMENSION: int = 256\n",
    "N_HIDDEN: int = 3\n",
    "\n",
    "def tensor(x: np.array, type=torch.float32, device=DEVICE) -> torch.Tensor:\n",
    "    return torch.as_tensor(x, dtype=type, device=device)\n",
    "\n",
    "\n",
    "def network(\n",
    "        in_dimension: int, \n",
    "        out_dimension: int, \n",
    "        hidden_dimension: int = 256, \n",
    "        n_hidden: int = 3) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_dimension (int): Dimension of the input layer.\n",
    "        hidden_dimension (int): Dimension of the hidden layers.\n",
    "        out_dimension (int): Dimension of the output layer.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed neural network model.\n",
    "    \"\"\"\n",
    "    shapes = [in_dimension] + [hidden_dimension] * n_hidden + [out_dimension]\n",
    "    layers = []\n",
    "    for i in range(len(shapes) - 2):\n",
    "        layers.append(nn.Linear(shapes[i], shapes[i+1]))\n",
    "        layers.append(nn.Mish())\n",
    "    layers.append(nn.Linear(shapes[-2], shapes[-1]))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dimension: int,\n",
    "            action_dimension: int,\n",
    "            hidden_dimension: int = HIDDEN_DIMENSION,\n",
    "            n_hidden: int = N_HIDDEN,\n",
    "    ):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.network = network(\n",
    "            state_dimension, 2 * action_dimension, hidden_dimension, n_hidden\n",
    "        )\n",
    "        self.action_dimension = action_dimension\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the Policy network. Should return mean and log_std of the policy distribution\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The input state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The tuple (mean, log_std) of the distribution corresponding to each action\n",
    "        \"\"\"\n",
    "        out = self.network(state)\n",
    "        mean, log_std = torch.split(out, self.action_dimension, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -10, 2)\n",
    "        return mean, log_std\n",
    "        \n",
    "\n",
    "    def pi(self, state: torch.Tensor) -> Normal:\n",
    "        \"\"\"\n",
    "        Computes the action distribution π(a|s) for a given state.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The input state.\n",
    "\n",
    "        Returns:\n",
    "            Categorical: The action distribution.\n",
    "        \"\"\"\n",
    "        mean, log_std = self(state)\n",
    "        std = log_std.exp()\n",
    "        return Normal(mean, std)\n",
    "\n",
    "    def action(self, state: np.ndarray, eval=False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects an action based on the policy without returning the log probability.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The input state.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The selected action.\n",
    "        \"\"\"\n",
    "        state = tensor(state)\n",
    "\n",
    "        policy = self.pi(state)\n",
    "        if eval:\n",
    "            action = policy.mean.cpu().numpy()\n",
    "        else:\n",
    "            action = policy.sample().cpu().numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectile_loss(diff, expectile=0.8):\n",
    "    return torch.abs(expectile-(diff<0).int())*torch.square(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 2452/3858 [00:13<00:07, 189.08it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 150\n",
    "EVAL_FREQ = 15\n",
    "LOAD_FROM_CKPT = True\n",
    "\n",
    "# These parameters should work fine but you may tune them if you want to\n",
    "hidden_dim: int = 256\n",
    "n_hidden: int = 2\n",
    "lr: float = 3e-4\n",
    "discount = 0.99\n",
    "alpha = 0.005\n",
    "exp_advantage_max = 100\n",
    "\n",
    "tau = 0.7\n",
    "beta = 3\n",
    "\n",
    "min_rew, max_rew = return_range(dataset, 1000)\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "sampler = D4RLSampler(dataset, 256, DEVICE)\n",
    "\n",
    "iql_policy = GaussianPolicy(state_dimension, action_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "policy_optimizer = Adam(iql_policy.parameters(), lr)\n",
    "policy_lr_schedule = CosineAnnealingLR(policy_optimizer, EPOCHS * len(sampler))\n",
    "\n",
    "v_critic = VNetwork(state_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "v_optimizer = Adam(v_critic.parameters(), lr)\n",
    "\n",
    "q_critic = QNetwork(state_dimension, action_dimension, hidden_dim, n_hidden).to(DEVICE)\n",
    "q_critic_target = copy.deepcopy(q_critic)\n",
    "q_critic_target.requires_grad_(False)\n",
    "q_optimizer = Adam(q_critic.parameters(), lr)\n",
    "\n",
    "means, stds, start_epoch = [], [], 0\n",
    "if os.path.exists('iql_checkpoint.pth') and LOAD_FROM_CKPT:\n",
    "    checkpoint = torch.load('iql_checkpoint.pth')\n",
    "\n",
    "    iql_policy.load_state_dict(checkpoint['iql_policy'])\n",
    "    policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "    v_critic.load_state_dict(checkpoint['v_critic'])\n",
    "    v_optimizer.load_state_dict(checkpoint['v_optimizer'])\n",
    "    q_critic.load_state_dict(checkpoint['q_critic'])\n",
    "    q_critic_target.load_state_dict(checkpoint['q_critic_target'])\n",
    "    q_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    means = checkpoint['means']\n",
    "    stds = checkpoint['stds']\n",
    "    \n",
    "    print(f'Resuming run from epoch {start_epoch}')\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_q_loss = total_v_loss = total_policy_loss = count = 0\n",
    "    policy_losses = []\n",
    "    # for batch in tqdm(dataloader):\n",
    "    for batch in tqdm(sampler):\n",
    "        state = batch['state'].to(DEVICE)\n",
    "        next_state = batch['next_state'].to(DEVICE)\n",
    "        action = batch['action'].to(DEVICE)\n",
    "        reward = einops.rearrange(batch['reward'], 'b -> b 1').to(DEVICE)\n",
    "        reward = reward / (max_rew - min_rew) * 1000\n",
    "        not_done = einops.rearrange(batch['not_done'], 'b -> b 1').to(DEVICE)\n",
    "\n",
    "        v_loss = expectile_loss(torch.minimum(*q_critic_target(state,action))-v_critic(state), expectile=tau).mean()\n",
    "        v_optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        v_optimizer.step()\n",
    "\n",
    "        target = reward+not_done*discount*v_critic(next_state)\n",
    "        q_values = q_critic(state,action)\n",
    "        q_loss = F.mse_loss(q_values[0], target) + F.mse_loss(q_values[1], target)\n",
    "        q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        q_optimizer.step()\n",
    "        for var, var_target in zip(q_critic.parameters(), q_critic_target.parameters()):\n",
    "            var_target.data = alpha * var.data + (1.0 - alpha) * var_target.data\n",
    "\n",
    "        mean, log_std = iql_policy(state)\n",
    "        \n",
    "        advantage = torch.minimum(*q_critic_target(state,action))-v_critic(state)\n",
    "        weight = torch.clamp(torch.exp(beta*advantage),max=exp_advantage_max)\n",
    "        policy_loss = (weight*F.gaussian_nll_loss(mean, action, log_std.exp()**2, reduction='none')).mean()\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        policy_lr_schedule.step()\n",
    "        total_v_loss += v_loss.item()\n",
    "        total_q_loss += q_loss.item()\n",
    "        total_policy_loss += policy_loss.item()\n",
    "        count += 1\n",
    "        \n",
    "    if (epoch + 1) % EVAL_FREQ == 0:\n",
    "        rew_mean, rew_std = eval_policy(iql_policy, environment_name=ENVIRONMENT_NAME, eval_episodes=50)\n",
    "        print(f'Epoch: {epoch + 1}. Q Loss: {total_q_loss / count:.4f}. V Loss: {total_v_loss / count:.4f}. P Loss: {total_policy_loss / count:.4f}. Reward: {rew_mean:.4f} +/- {rew_std:.4f}')\n",
    "        means.append(rew_mean)\n",
    "        stds.append(rew_std)\n",
    "\n",
    "    # Save a checkpoint so that you can resume training if it crashes\n",
    "    checkpoint = {\n",
    "        'iql_policy': iql_policy.state_dict(),\n",
    "        'policy_optimizer': policy_optimizer.state_dict(),\n",
    "        'v_critic': v_critic.state_dict(),\n",
    "        'v_optimizer': v_optimizer.state_dict(),\n",
    "        'q_critic': q_critic.state_dict(),\n",
    "        'q_critic_target': q_critic_target.state_dict(),\n",
    "        'q_optimizer': q_optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'means': means,\n",
    "        'stds': stds\n",
    "    }\n",
    "    torch.save(checkpoint, 'iql_checkpoint.pth')\n",
    "\n",
    "epochs = np.arange(EVAL_FREQ, EPOCHS + EVAL_FREQ, step=EVAL_FREQ)\n",
    "plot_returns(means, stds, 'Implicit Q Learning', goal=0.4, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cs8803drl_hw2]",
   "language": "python",
   "name": "conda-env-.conda-cs8803drl_hw2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
